import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import xgboost as xgb
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, r2_score

# ===================================================================
# 0. ë°ì´í„° ì¤€ë¹„
# ===================================================================
print("--- 0. ë°ì´í„° ì¤€ë¹„ ì‹œì‘ ---")
# ì‚¬ìš©í•˜ì‹œëŠ” íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
file_path = 'Screwed-Backend/data/MERGED_file.csv' 
df = pd.read_csv(file_path)

columns_to_drop = ['CODE', 'YEAR', 'LAT', 'LON', 'MAIZE', 'WHEAT', 'RICE']
df = df.drop(columns=columns_to_drop, errors='ignore')

if 'SOYBEAN' in df.columns:
    df = df.rename(columns={'SOYBEAN': 'yield'})
df['yield'] = df['yield'].fillna(0)
df.fillna(df.mean(), inplace=True)

num_features_lstm = df.shape[1] 

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df)
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)
print("--- 0. ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ---\n")


# ===================================================================
# 1. ëª¨ë¸ 1: ìˆ˜í™• ì—¬ë¶€ ì˜ˆì¸¡ (XGBoost ë¶„ë¥˜ ëª¨ë¸)
# ===================================================================
print("--- 1. XGBoost ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
X_clf = df.drop(['yield', 'MONTH'], axis=1)
y_clf = (df['yield'] > 0).astype(int)

X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf)

# --- 1-1. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ì„ íƒ ---
print("--- 1-1. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ìƒìœ„ 10ê°œ ì„ íƒ ---")
# ê°€ì¤‘ì¹˜ ê³„ì‚°
scale_pos_weight = (y_clf_train == 0).sum() / (y_clf_train == 1).sum()

# ì´ˆê¸° ëª¨ë¸ì„ í•™ìŠµí•˜ì—¬ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°
initial_model = xgb.XGBClassifier(objective='binary:logistic', scale_pos_weight=scale_pos_weight, eval_metric='logloss')
initial_model.fit(X_clf_train, y_clf_train)

# ì¤‘ìš”ë„ë¥¼ DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ ì •ë ¬
feature_importances = pd.DataFrame({
    'feature': X_clf_train.columns,
    'importance': initial_model.feature_importances_
}).sort_values('importance', ascending=False)

# ìƒìœ„ 10ê°œ íŠ¹ì„± ì´ë¦„ ì¶”ì¶œ
top_10_features = feature_importances.head(10)['feature'].tolist()
print("âœ… ì„ íƒëœ ìƒìœ„ 10ê°œ íŠ¹ì„±:")
print(top_10_features)

# ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ ê°€ì§„ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ìƒì„±
X_train_top10 = X_clf_train[top_10_features]
X_test_top10 = X_clf_test[top_10_features]


# --- 1-2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒëœ íŠ¹ì„± ì‚¬ìš©) ---
print("\n--- 1-2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ---")
param_grid = {'max_depth': [3, 5, 7], 'learning_rate': [0.1], 'n_estimators': [100, 200]}
grid_search = GridSearchCV(estimator=initial_model, param_grid=param_grid, scoring='f1', cv=3, verbose=0)
# ìƒìœ„ 10ê°œ íŠ¹ì„±ìœ¼ë¡œ íŠœë‹ ì§„í–‰
grid_search.fit(X_train_top10, y_clf_train)

best_classifier = grid_search.best_estimator_
print(f"âœ… ìµœì ì˜ íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")


# --- 1-3. ìµœì¢… ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ---
y_pred_clf = best_classifier.predict(X_test_top10)
print("\n--- [ìƒìœ„ 10ê°œ íŠ¹ì„± ì‚¬ìš© ì‹œ] ë¶„ë¥˜ ëª¨ë¸ ì •ë°€ ì§„ë‹¨ ë¦¬í¬íŠ¸ ---")
print(classification_report(y_clf_test, y_pred_clf, target_names=['No_Harvest (0)', 'Harvest (1)']))
print("--- 1. XGBoost ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ---\n")


# ===================================================================
# 2. ëª¨ë¸ 2: ìƒì‚°ëŸ‰ ì˜ˆì¸¡ (íšŒê·€ LSTM ëª¨ë¸)
# ===================================================================
print("--- 2. íšŒê·€ LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---")
harvest_only_df = scaled_df[df['yield'].values > 0]
X_reg, y_reg = [], []
for i in range(len(harvest_only_df) - 1):
    X_reg.append(harvest_only_df.iloc[i].values)
    y_reg.append(harvest_only_df.iloc[i + 1].values)

if len(X_reg) > 0:
    X_reg = np.array(X_reg).reshape(-1, 1, num_features_lstm)
    y_reg = np.array(y_reg)
    X_reg_tensor = torch.from_numpy(X_reg).float()
    y_reg_tensor = torch.from_numpy(y_reg).float()

    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
            self.fc = nn.Linear(hidden_size, output_size)
        def forward(self, x):
            lstm_out, _ = self.lstm(x)
            return self.fc(lstm_out[:, -1, :])

    regressor = LSTMModel(input_size=num_features_lstm, hidden_size=50, output_size=num_features_lstm)
    criterion_reg = nn.MSELoss()
    optimizer_reg = torch.optim.Adam(regressor.parameters(), lr=0.001)

    epochs = 100
    for epoch in range(epochs):
        outputs = regressor(X_reg_tensor)
        loss = criterion_reg(outputs, y_reg_tensor)
        optimizer_reg.zero_grad()
        loss.backward()
        optimizer_reg.step()
    print(f"LSTM ìµœì¢… Loss: {loss.item():.4f}")
else:
    regressor = None
    print("ê²½ê³ : LSTM ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜í™• ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
print("--- 2. íšŒê·€ LSTM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ---\n")


# ===================================================================
# 3. í†µí•© ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ (ê°•í™” ë²„ì „)
# ===================================================================
print("--- 3. í†µí•© ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì‹œì‘ ---")
results_list = []

if regressor is not None:
    regressor.eval()

with torch.no_grad():
    for i in range(len(X_clf_test)):
        # ë¶„ë¥˜ ì˜ˆì¸¡ì—ëŠ” ìƒìœ„ 10ê°œ íŠ¹ì„±ë§Œ ì‚¬ìš©
        input_clf_df_top10 = X_test_top10.iloc[[i]]
        
        predicted_harvest = best_classifier.predict(input_clf_df_top10)[0]
        harvest_probability = best_classifier.predict_proba(input_clf_df_top10)[0][1]

        predicted_yield = 0.0
        if predicted_harvest == 1 and regressor is not None:
            original_data_index = input_clf_df_top10.index[0]
            input_reg_np = scaled_df.iloc[original_data_index].values
            input_reg = torch.from_numpy(input_reg_np).float()
            next_state_scaled = regressor(input_reg.reshape(1, 1, num_features_lstm))
            next_state_original = scaler.inverse_transform(next_state_scaled.numpy())
            yield_col_index = df.columns.get_loc('yield')
            predicted_yield = next_state_original[0, yield_col_index]

        original_data_index = input_clf_df_top10.index[0]
        original_row = df.iloc[original_data_index]
        month = original_row['MONTH']
        actual_yield = original_row['yield']
        actual_harvest = y_clf_test.iloc[i]
        
        results_list.append({
            'Month': int(month),
            'Actual_Harvest': actual_harvest,
            'Predicted_Harvest': predicted_harvest,
            'Harvest_Probability(%)': harvest_probability * 100,
            'Actual_Yield': actual_yield,
            'Predicted_Yield': predicted_yield,
        })

results_df = pd.DataFrame(results_list)
print("\n--- ğŸ“ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸í‘œ (ìƒìœ„ 20ê°œ) ---")
pd.options.display.float_format = '{:.2f}'.format
print(results_df.head(20).to_string())

print("\n\n" + "="*50)
print("--- ğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½ ---")
print("="*50)
print("\n[ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½]")
print(classification_report(results_df['Actual_Harvest'], results_df['Predicted_Harvest'], target_names=['No_Harvest (0)', 'Harvest (1)']))
harvest_samples_df = results_df[results_df['Actual_Harvest'] == 1]
if not harvest_samples_df.empty:
    mae = mean_absolute_error(harvest_samples_df['Actual_Yield'], harvest_samples_df['Predicted_Yield'])
    r2 = r2_score(harvest_samples_df['Actual_Yield'], harvest_samples_df['Predicted_Yield'])
    print("\n[íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ (ì‹¤ì œ ìˆ˜í™•ì´ ìˆì—ˆë˜ ê²½ìš°)]")
    print(f"í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE): {mae:.2f}")
    print(f"R-squared (RÂ²): {r2:.4f}")
else:
    print("\n[íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½]")
    print("í‰ê°€í•  ìˆ˜í™• ìƒ˜í”Œì´ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
print("\n--- 3. í†µí•© ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ---")