# Screwed-Backend

**NASA Space Apps Challenge 2024** - Agricultural Climate Simulation Game Backend

How to use: 
API: https://api.tedxkaist.org/api/v1/docs#

A FastAPI-based backend service that combines climate prediction ML models with LLM-driven natural language processing to create an educational farming simulation game. Players make agricultural decisions through natural language commands, and the system predicts crop yields based on NASA climate data.

---

## Table of Contents

- [Overview](#overview)
- [System Architecture](#system-architecture)
- [API Reference](#api-reference)
- [LLM Components](#llm-components)
- [ML Models](#ml-models)
- [Data Models](#data-models)
- [Game Scenarios & Flow](#game-scenarios--flow)
- [Setup & Deployment](#setup--deployment)
- [Development](#development)

---

## Overview

### Tech Stack

- **Framework**: FastAPI (Python 3.11)
- **Database**: PostgreSQL (hosted on GCP)
- **Task Queue**: Celery + Redis
- **LLM**: Google Gemini (via `google-generativeai`)
- **ML**: PyTorch (climate prediction) + LightGBM (yield prediction)
- **Deployment**: Docker Compose + nginx-proxy + Let's Encrypt

### Key Features

1. **Turn-based Timeline System**: Branching game timelines with optimistic concurrency control
2. **Natural Language Commands**: LLM-powered command parsing and validation
3. **Climate Prediction**: LSTM-based climate forecasting using NASA data (26 variables)
4. **Yield Prediction**: LightGBM models for crop yield estimation (corn, rice, wheat, soybean)
5. **Async Processing**: Celery workers handle long-running ML/LLM tasks
6. **Multi-timeline Support**: Players can fork timelines to explore different strategies

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Unity     â”‚ (Game Client)
â”‚  Frontend   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP/REST
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FastAPI Backend                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  API Layer (src/app/api/v1/)        â”‚   â”‚
â”‚  â”‚  - Turns, Commands, Outputs         â”‚   â”‚
â”‚  â”‚  - Sessions, Health                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Service Layer (src/app/services/)  â”‚   â”‚
â”‚  â”‚  - turn_service                     â”‚   â”‚
â”‚  â”‚  - command_service / command_task   â”‚   â”‚
â”‚  â”‚  - output_service / output_task     â”‚   â”‚
â”‚  â”‚  - prediction_service               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Data Layer (src/app/models/)       â”‚   â”‚
â”‚  â”‚  - Turn, Command, Output, Session   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL  â”‚      â”‚  Redis + Celery  â”‚
â”‚   Database   â”‚      â”‚   Worker Queue   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  External Services   â”‚
                   â”‚  - Google Gemini     â”‚
                   â”‚  - ML Models (local) â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow

**Synchronous (CRUD operations)**:
```
Client â†’ FastAPI Endpoint â†’ Service Layer â†’ Database â†’ Response
```

**Asynchronous (LLM/ML tasks)**:
```
Client â†’ FastAPI Endpoint â†’ Service Layer â†’ Celery Task (queued)
       â† Task ID returned

Celery Worker â†’ LLM/ML Processing â†’ Update Database â†’ Store Result
```

---

## API Reference

All endpoints are prefixed with `/api/v1`

### ğŸ“Š Turns (Game Timeline Management)

Turns represent individual months in the game timeline with branching support.

| Method | Endpoint | Description | Request Body | Response |
|--------|----------|-------------|--------------|----------|
| `POST` | `/turns` | Create new turn | `TurnCreate` | `TurnOut` |
| `GET` | `/turns/{turn_id}` | Get single turn | - | `TurnOut` |
| `GET` | `/turns` | List turns (filterable) | Query: `branch_id`, `parent_id`, `month`, `state`, `limit`, `cursor` | `List[TurnOut]` |
| `PATCH` | `/turns/{turn_id}/stats` | Update turn statistics | `TurnUpdateStats` | `TurnOut` |
| `POST` | `/turns/{turn_id}/state/{next_state}` | Transition turn state | - | `TurnOut` |
| `POST` | `/turns/{turn_id}/children` | Create child turn (same branch) | `TurnCreate` | `TurnOut` |
| `POST` | `/turns/{turn_id}/branch` | Create new branch | `TurnCreate` | `TurnOut` |
| `GET` | `/turns/{turn_id}/tree` | Get full turn tree | Query: `session_id`, `max_nodes`, `use_recursive_cte` | `TreeStructure` |
| `POST` | `/crash` | Handle concurrent turn creation | `CrashNodeInput` + Header: `If-Match` | `TurnOut` (200/201) or `ConflictOut` (409) |

**Turn States** (State Machine):
```
DRAFT â†’ BRIEFED â†’ COMMAND_PENDING â†’ VALIDATING â†’ VALIDATED â†’
COST_ESTIMATED â†’ BUDGET_OK â†’ SIMULATED â†’ APPLIED
                    â†“
                REJECTED â†’ COMMAND_PENDING (retry)
```

### ğŸ® Commands (Player Instructions)

Commands are natural language instructions that players give each turn.

| Method | Endpoint | Description | Request Body | Response |
|--------|----------|-------------|--------------|----------|
| `POST` | `/turns/{turn_id}/commands` | Create command (triggers async validation) | `CommandCreate` | `CommandCreateOut` |
| `GET` | `/turns/{turn_id}/commands` | List commands for turn | Query: `limit`, `cursor` | `List[CommandOut]` |
| `GET` | `/turns/{turn_id}/commands/{cmd_id}` | Get single command | - | `CommandOut` |
| `POST` | `/turns/{turn_id}/commands/{cmd_id}/validate` | Set validation result | `CommandValidateIn` | `CommandOut` |
| `POST` | `/turns/{turn_id}/commands/{cmd_id}/estimate-cost` | Set cost estimate | `CommandCostIn` | `CommandOut` |

**Example Command**:
```json
{
  "text": "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ê³  ë“œë¦½ ê´€ê°œ ì„¤ì¹˜í•´ì¤˜",
  "payload": {}
}
```

### ğŸ“ˆ Outputs (Predictions & Results)

Outputs contain climate/yield predictions generated from commands.

| Method | Endpoint | Description | Request Body | Response |
|--------|----------|-------------|--------------|----------|
| `POST` | `/commands/{command_id}/outputs` | Create output (triggers async prediction) | - | `OutputIdOut` |
| `GET` | `/outputs/{output_id}` | Get output | - | `OutputOut` |
| `GET` | `/turns/{turn_id}/outputs` | List outputs for turn | Query: `limit`, `cursor` | `List[OutputOut]` |
| `POST` | `/outputs/{output_id}/apply` | Apply output to game state | - | Status message |

**Output States**: `PENDING` â†’ `RUNNING` â†’ `COMPLETE` / `FAILED`

**Output Kinds**: `DENIED`, `ANSWER`, `PROCESSED`

### ğŸ¯ Sessions (Game Sessions)

Sessions group multiple turns into a single gameplay session.

| Method | Endpoint | Description | Request Body | Response |
|--------|----------|-------------|--------------|----------|
| `POST` | `/sessions` | Create session | `SessionCreate` | `SessionOut` |
| `GET` | `/sessions/{session_id}` | Get session | - | `SessionOut` |
| `GET` | `/sessions` | List sessions | Query: `limit`, `cursor` | `List[SessionOut]` |
| `PATCH` | `/sessions/{session_id}` | Update session | `SessionUpdate` | `SessionOut` |
| `DELETE` | `/sessions/{session_id}` | Delete session | - | 204 No Content |

### ğŸ¥ Health

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |

---

## LLM Components

The system uses **Google Gemini** (`gemini-flash-latest`) for natural language processing with config-driven prompts.

### LLM Configuration Files

Located in [`src/app/utils/`](src/app/utils/):

| File | Purpose | Input | Output |
|------|---------|-------|--------|
| `normalizeUserinput.json` | Normalize typos, slang, extract action count | User text | `{normalized_text, action_count}` |
| `questionTypeChecker.json` | Classify input type | Normalized text | `{type: "Q"/"I"/"O"}` (Question/Instruction/Other) |
| `procedureAnalyzer.json` | Parse instruction into structured command | Instruction text | `{intent, crop, area, params...}` |
| `feedbackGenerator.json` | Generate narrative feedback | Simulation results | Natural language response |

### LLM Functions

Implemented in [`src/app/services/llm.py`](src/app/services/llm.py):

```python
# 1. Normalize user input
normalizeInput(message: str) -> dict
# Returns: {"normalized_text": str, "action_count": int}

# 2. Determine question type
determineQuestionType(normalizedData: dict) -> dict
# Returns: {"type": "Q" | "I" | "O"}

# 3. Analyze procedure (parse instruction)
procedureAnalyzer(sentence: str) -> dict
# Returns: {"intent": str, "crop": str, "target_area": str, ...}

# 4. Question handler (answer questions)
questionHandler(question_text: str) -> dict
# Returns: {"final_response": str, "status": "ANSWERED" | "ERROR"}

# 5. Generate feedback (after simulation)
generateFeedback(simulation_result: dict) -> str
```

### LLM Processing Flow

```
User Input: "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ê³  ë“œë¦½ ê´€ê°œ ì„¤ì¹˜í•´ì¤˜"
    â†“
normalizeInput()
    â†“
{"normalized_text": "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ê³  ë“œë¦½ ê´€ê°œ ì„¤ì¹˜.", "action_count": 2}
    â†“
determineQuestionType()
    â†“
{"type": "I"}  (Instruction)
    â†“
Check action_count
    â†“
VIOLATION: "í•œ í„´ì— í•˜ë‚˜ì˜ ì¡°ì¹˜ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
```

**Valid Single Action Example**:
```
User Input: "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ì–´ì¤˜"
    â†“
normalizeInput() â†’ {"normalized_text": "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ê¸°", "action_count": 1}
    â†“
determineQuestionType() â†’ {"type": "I"}
    â†“
procedureAnalyzer() â†’ {"intent": "ì‹¬ê¸°", "crop": "ì˜¥ìˆ˜ìˆ˜", "area": 100, "unit": "ì—ì´ì»¤"}
    â†“
Execute simulation â†’ Update game state
    â†“
generateFeedback() â†’ "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹¬ì—ˆìŠµë‹ˆë‹¤..."
```

---

## ML Models

### 1. Climate Prediction Model

**Location**: [`src/app/services/climate_inference.py`](src/app/services/climate_inference.py)

**Architecture**: FNN â†’ ResLSTM Blocks â†’ Attention Pooling â†’ Gaussian Head

**Model Type**: PyTorch (CPU-only inference)

**Artifacts** (in [`artifacts/predictClimate/`](artifacts/predictClimate/)):
- `model.pt` (6.4 MB) - PyTorch model weights
- `normalizer_stats.json` (8.2 MB) - Feature normalization statistics
- `calibration.json` - Per-feature calibration parameters

**Input**:
- Recent L timesteps (default: 6 months) of F climate features
- Metadata: month, latitude, longitude
- NASA climate variables (26 features): T2M, RH2M, PRECTOTCORR, etc.

**Output**:
- Gaussian predictions: `[mu, logvar]` for each climate variable
- Calibrated using `log_t_f` and `scalar_s` from calibration.json

**Model Class**:
```python
class FNN_ResLSTM_AttnHead(nn.Module):
    - Pre-FNN: Linear â†’ GELU â†’ Dropout â†’ Linear
    - ResLSTM Blocks (3 layers): LSTM with residual connections
    - Attention Pooling: Self-attention over timesteps
    - Gaussian Head: Output [mu, logvar] for uncertainty estimation
```

### 2. Yield Prediction Models

**Location**: [`src/app/services/yield_inference.py`](src/app/services/yield_inference.py)

**Model Type**: LightGBM (gradient boosting)

**Crops Supported**: Corn (maize), Rice, Wheat, Soybean

**Artifacts** (in [`artifacts/yieldInference/{crop}/`](artifacts/yieldInference/)):
- Each crop folder contains:
  - `model.pt` - LightGBM booster (PyTorch serialized)
  - `feature_names.json` - Expected input features

**Input Features** (generated by [`featurizer_yield.py`](src/app/services/featurizer_yield.py)):
- Climate statistics (mean, std, min, max for temperature, precipitation, humidity)
- Location metadata (latitude, longitude, region)
- Crop-specific parameters (planting date, variety)

**Output**:
- Crop yield prediction (tons per hectare)
- Uncertainty estimates (optional)

**Usage**:
```python
from app.services.yield_inference import YieldPredictorLGBM

predictor = YieldPredictorLGBM("artifacts/yieldInference/maize")
predictions = predictor.predict(feature_dataframe)
```

### 3. Prediction Service Integration

**Location**: [`src/app/services/prediction_service.py`](src/app/services/prediction_service.py)

Orchestrates climate + yield predictions:

```python
def run_prediction(
    variables: List[str],
    location: Dict[str, float],
    horizon_days: int,
    context: Dict[str, Any],
    user_text: str | None
) -> Tuple[Dict, Dict, Dict]:
    # Returns: (prediction, impact, delta_stats)
```

**Current Implementation**: Dummy/placeholder that generates synthetic data. Replace with actual climate/yield model calls.

---

## Data Models

### Database Schema

**Turn** ([`src/app/models/turn.py`](src/app/models/turn.py)):
```python
{
  id: str              # t_{uuid}
  parent_id: str       # Foreign key to parent turn
  branch_id: str       # Timeline branch identifier
  session_id: str      # Foreign key to session
  month: str           # YYYY-MM format
  state: TurnState     # State machine enum
  stats: JSONB         # {climate, yield, env, money, notes}
  created_at: datetime
  updated_at: datetime
}
```

**Command** ([`src/app/models/command.py`](src/app/models/command.py)):
```python
{
  id: str              # c_{uuid}
  turn_id: str         # Foreign key to turn
  text: str            # Natural language instruction
  validity: JSONB      # {is_valid, score, reasons[]}
  cost: JSONB          # {estimate, currency, breakdown[]}
  payload: JSON        # Additional metadata
  created_at: str
}
```

**Output** ([`src/app/models/output.py`](src/app/models/output.py)):
```python
{
  id: str              # o_{uuid}
  command_id: str      # Foreign key to command
  turn_id: str         # Foreign key to turn
  state: str           # PENDING/RUNNING/COMPLETE/FAILED
  kind: str            # DENIED/ANSWER/PROCESSED
  answer: str          # Natural language response
  impact: JSONB        # {top_features: [{name, contrib}]}
  prediction: JSONB    # {target, yhat[], ts_start, freq, ...}
  delta_stats: JSONB   # {baseline, pred_mean, delta}
  models: JSONB        # {climate_model, yield_model}
  assumptions: JSON[]  # Model assumptions
  denied_reasons: JSON[]
  created_at: str
  completed_at: str
}
```

**Session** ([`src/app/models/session.py`](src/app/models/session.py)):
```python
{
  id: str              # s_{uuid}
  title: str
  root_turn_id: str    # Reference to first turn
  meta: JSONB          # Custom metadata
  created_at: datetime
  updated_at: datetime
}
```

### Pydantic Schemas

Request/response schemas in [`src/app/schemas/`](src/app/schemas/):

- `TurnCreate`, `TurnOut`, `TurnUpdateStats`, `TurnState`
- `CommandCreate`, `CommandOut`, `CommandValidateIn`, `CommandCostIn`
- `OutputCreateIn`, `OutputOut`, `OutputIdOut`
- `SessionCreate`, `SessionUpdate`, `SessionOut`
- `CrashNodeInput`, `ConflictOut` (for crash handling)

---

## Game Scenarios & Flow

### Typical Gameplay Loop

```
1. Client creates session
   POST /sessions â†’ {session_id: "s_abc123"}

2. Client creates initial turn
   POST /turns â†’ {id: "t_root_b_xyz", state: "DRAFT", month: "2024-01"}

3. Player enters command
   POST /turns/t_root_b_xyz/commands
   Body: {text: "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ì–´ì¤˜"}
   â†’ Returns: {command_id: "c_def456", output_id: "o_ghi789", task_id: "celery-task-id"}

4. System processes command asynchronously
   - Celery worker runs command_task
   - LLM validates command (normalizeInput â†’ determineQuestionType â†’ procedureAnalyzer)
   - Updates command.validity in database
   - Triggers output generation

5. System generates prediction asynchronously
   - Celery worker runs output_task
   - Runs climate prediction (climate_inference.py)
   - Runs yield prediction (yield_inference.py)
   - Stores results in output.prediction, output.impact, output.delta_stats

6. Client polls for completion
   GET /outputs/o_ghi789
   â†’ {state: "COMPLETE", kind: "PROCESSED", prediction: {...}, impact: {...}}

7. Client applies output
   POST /outputs/o_ghi789/apply
   â†’ Updates turn state to APPLIED

8. Client creates next turn
   POST /turns/t_root_b_xyz/children
   Body: {month: "2024-02"}
   â†’ {id: "t_child_1", parent_id: "t_root_b_xyz"}

9. Repeat from step 3
```

### Branching Scenario

```
Timeline A:
t_root â†’ t_child_1 â†’ t_child_2 (player: plant corn)
                         â†“
                    Player wants to try wheat instead
                         â†“
                    POST /turns/t_child_1/branch
                         â†“
Timeline B:          t_branch_1 (player: plant wheat)
```

### Crash Handling (Optimistic Concurrency)

```
Scenario: Two clients try to extend same timeline simultaneously

Client 1                          Client 2
   |                                 |
   POST /crash                       POST /crash
   current_id: t_xyz                 current_id: t_xyz
   next: {...}                       next: {...}
   |                                 |
   âœ“ Returns 201 (created)           âœ— Returns 409 (conflict)
   t_new_1                           {latest_id: t_new_1, latest_etag: "..."}
                                     |
                                     Client decides: FORK or RETRY
                                     |
                                     POST /crash (with resolution: FORK)
                                     |
                                     âœ“ Returns 201 (new branch created)
```

### Command Validation Scenarios

**Valid Single Action**:
```
Input: "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ê¸°"
â†’ action_count: 1
â†’ procedureAnalyzer: {intent: "ì‹¬ê¸°", crop: "ì˜¥ìˆ˜ìˆ˜", area: 100}
â†’ Status: VALIDATED
â†’ Execute simulation
```

**Invalid Multiple Actions**:
```
Input: "ì˜¥ìˆ˜ìˆ˜ ì‹¬ê³  ë°€ ìˆ˜í™•í•´ì¤˜"
â†’ action_count: 2
â†’ Status: VIOLATION
â†’ Response: "í•œ í„´ì— í•˜ë‚˜ì˜ ì¡°ì¹˜ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
```

**Invalid Delegation**:
```
Input: "ì•Œì•„ì„œ í•´ì¤˜"
â†’ procedureAnalyzer: {intent: "ìœ„ì„_ë¶ˆê°€"}
â†’ Status: VIOLATION
â†’ Response: "êµ¬ì²´ì ì¸ ì‘ë¬¼, ì§€ì—­, ê´€ê°œ ë°©ì‹ì„ ì§€ì •í•´ì£¼ì„¸ìš”"
```

**Question Handling**:
```
Input: "í˜„ì¬ ìˆ˜ë¶„ ìŠ¤íŠ¸ë ˆìŠ¤ëŠ” ì–´ë•Œ?"
â†’ questionType: "Q"
â†’ questionHandler: Query game state
â†’ Response: "í˜„ì¬ ìˆ˜ë¶„ ìŠ¤íŠ¸ë ˆìŠ¤ ì§€ìˆ˜ëŠ” 0.25ì…ë‹ˆë‹¤..."
```

---

## Setup & Deployment

### Prerequisites

- Docker & Docker Compose
- PostgreSQL database (or use provided GCP instance)
- Google Gemini API key

### Environment Variables

Create `.env` file:

```bash
# Database
DATABASE_URL=postgresql+psycopg://user:pass@host:5432/dbname

# Celery/Redis
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# LLM
GOOGLE_API_KEY=your-gemini-api-key

# Deployment (optional)
VIRTUAL_HOST=api.yourdomain.com
LETSENCRYPT_HOST=api.yourdomain.com
LETSENCRYPT_EMAIL=your-email@example.com
```

### Docker Deployment

```bash
# Build and start all services
docker-compose up -d --build

# Services started:
# - backend (FastAPI on port 8000)
# - worker (Celery worker)
# - redis (message broker)
# - flower (Celery monitoring on port 5555)
# - nginx-proxy (reverse proxy on port 80/443)
# - letsencrypt (SSL certificate management)

# View logs
docker-compose logs -f backend
docker-compose logs -f worker

# Stop services
docker-compose down
```

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Run backend
PYTHONPATH=./src uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Run Celery worker (in separate terminal)
PYTHONPATH=./src celery -A app.core.celery_app worker -Q validation -l INFO

# Run Redis (or use Docker)
docker run -d -p 6379:6379 redis:7
```

### Accessing Services

- **API Docs**: http://localhost:8000/api/v1/docs
- **Flower (Celery Monitor)**: http://localhost:5555
- **Production API**: https://api.tedxkaist.org/api/v1/docs
- **Production Flower**: https://flower.api.tedxkaist.org

---

## Development

### Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/v1/          # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ turns.py
â”‚   â”‚   â”‚   â”œâ”€â”€ commands.py
â”‚   â”‚   â”‚   â”œâ”€â”€ outputs.py
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.py
â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â”œâ”€â”€ core/            # Configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â””â”€â”€ celery_app.py
â”‚   â”‚   â”œâ”€â”€ db/              # Database
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â””â”€â”€ session.py
â”‚   â”‚   â”œâ”€â”€ models/          # SQLAlchemy models
â”‚   â”‚   â”‚   â”œâ”€â”€ turn.py
â”‚   â”‚   â”‚   â”œâ”€â”€ command.py
â”‚   â”‚   â”‚   â”œâ”€â”€ output.py
â”‚   â”‚   â”‚   â””â”€â”€ session.py
â”‚   â”‚   â”œâ”€â”€ schemas/         # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ services/        # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ turn_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ command_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ command_task.py      # Celery task
â”‚   â”‚   â”‚   â”œâ”€â”€ output_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ output_task.py       # Celery task
â”‚   â”‚   â”‚   â”œâ”€â”€ prediction_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ prediction_task.py   # Celery task
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py               # LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ climate_inference.py # Climate ML model
â”‚   â”‚   â”‚   â”œâ”€â”€ yield_inference.py   # Yield ML model
â”‚   â”‚   â”‚   â””â”€â”€ featurizer_yield.py
â”‚   â”‚   â”œâ”€â”€ utils/           # LLM config files
â”‚   â”‚   â”‚   â”œâ”€â”€ normalizeUserinput.json
â”‚   â”‚   â”‚   â”œâ”€â”€ questionTypeChecker.json
â”‚   â”‚   â”‚   â”œâ”€â”€ procedureAnalyzer.json
â”‚   â”‚   â”‚   â””â”€â”€ feedbackGenerator.json
â”‚   â”‚   â””â”€â”€ main.py          # FastAPI app
â”‚   â””â”€â”€ migrations/          # Database migrations
â”œâ”€â”€ artifacts/               # ML model artifacts
â”‚   â”œâ”€â”€ predictClimate/
â”‚   â”‚   â”œâ”€â”€ model.pt
â”‚   â”‚   â”œâ”€â”€ normalizer_stats.json
â”‚   â”‚   â””â”€â”€ calibration.json
â”‚   â””â”€â”€ yieldInference/
â”‚       â”œâ”€â”€ maize/
â”‚       â”œâ”€â”€ rice/
â”‚       â”œâ”€â”€ soybean/
â”‚       â””â”€â”€ wheat/
â”œâ”€â”€ ml/                      # ML training scripts
â”‚   â”œâ”€â”€ LMmodel.py
â”‚   â””â”€â”€ data_flaten.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Adding New Features

**1. New API Endpoint**:
```bash
# 1. Create schema (src/app/schemas/foo.py)
# 2. Create model (src/app/models/foo.py)
# 3. Create service (src/app/services/foo_service.py)
# 4. Create endpoint (src/app/api/v1/foo.py)
# 5. Register in src/app/api/v1/__init__.py
```

**2. New Celery Task**:
```bash
# 1. Create task file (src/app/services/my_task.py)
# 2. Add to celery.conf.imports in src/app/core/celery_app.py
# 3. Decorate function with @celery.task()
```

**3. New LLM Prompt**:
```bash
# 1. Create config JSON in src/app/utils/
# 2. Add function in src/app/services/llm.py using _call_gemini_model()
```

### Database Migrations

Currently using auto-creation via `Base.metadata.create_all()`. For production, set up Alembic:

```bash
# Initialize Alembic (if not done)
alembic init migrations

# Create migration
alembic revision --autogenerate -m "description"

# Apply migration
alembic upgrade head
```

### Testing

```bash
# Run tests (add pytest to requirements.txt)
pytest src/app/services/test_climate_inference.py
pytest src/app/services/test_yield_inference.py
```

---

## API Examples

### Create Session and Play

```bash
# 1. Create session
curl -X POST http://localhost:8000/api/v1/sessions \
  -H "Content-Type: application/json" \
  -d '{"title": "My First Game"}'

# Response: {"id": "s_abc123", "title": "My First Game", ...}

# 2. Create initial turn
curl -X POST http://localhost:8000/api/v1/turns \
  -H "Content-Type: application/json" \
  -d '{"session_id": "s_abc123", "month": "2024-01"}'

# Response: {"id": "t_xyz789", "state": "DRAFT", ...}

# 3. Create command
curl -X POST http://localhost:8000/api/v1/turns/t_xyz789/commands \
  -H "Content-Type: application/json" \
  -d '{"text": "ì˜¥ìˆ˜ìˆ˜ 100ì—ì´ì»¤ ì‹¬ê¸°"}'

# Response: {"command_id": "c_def456", "output_id": "o_ghi789", "task_id": "..."}

# 4. Check output status
curl http://localhost:8000/api/v1/outputs/o_ghi789

# Response: {"state": "COMPLETE", "prediction": {...}, "impact": {...}}

# 5. Apply output
curl -X POST http://localhost:8000/api/v1/outputs/o_ghi789/apply

# 6. Create next turn
curl -X POST http://localhost:8000/api/v1/turns/t_xyz789/children \
  -H "Content-Type: application/json" \
  -d '{"month": "2024-02"}'
```

---

## Contributors

NASA Space Apps Challenge 2024 Team

## License

[Specify license]

---

## Resources

- **FastAPI Docs**: https://fastapi.tiangolo.com/
- **Celery Docs**: https://docs.celeryq.dev/
- **Google Gemini API**: https://ai.google.dev/docs
- **PyTorch**: https://pytorch.org/
- **LightGBM**: https://lightgbm.readthedocs.io/
