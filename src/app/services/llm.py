import google.generativeai as genai
import os
import numpy as np
import pandas as pd

# --- ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ íŒŒì¼ì€ í”„ë¡œì íŠ¸ ë‚´ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤) ---

from pathlib import Path
import json

# íŒŒì¼ ëª¨ë“ˆ
from app.services.climate_inference import ClimatePredictor
from app.services.yield_inference import MAIZE_PredictorLGBM, WHEAT_PredictorLGBM, RICE_PredictorLGBM, SOYBEAN_PredictorLGBM

# í˜„ì¬ íŒŒì¼: .../src/app/services/XXX.py
HERE = Path(__file__).resolve()
APP_DIR = HERE.parents[1]          # .../src/app
UTILS_DIR = APP_DIR / "utils"      # .../src/app/utils

# íŒŒì¼ ê²½ë¡œ ì •ì˜ (services/utils ê°€ ì•„ë‹ˆë¼ utils)
questionTypeDeterminerPath = str(UTILS_DIR / "questionTypeChecker.json")
normalizeUserinputPath    = str(UTILS_DIR / "normalizeUserinput.json")
procedureAnalyzerPath     = str(UTILS_DIR / "procedureAnalyzer.json")
feedbackGeneratorPath     = str(UTILS_DIR / "feedbackGenerator.json")

# ML ëª¨ë¸ ê´€ë ¨ íŒŒì¼ ê²½ë¡œ
ARTIFACTS_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "artifacts")
MODEL_PATH = os.path.join(ARTIFACTS_DIR, "model.pt")
NORM_PATH = os.path.join(ARTIFACTS_DIR, "normalizer_stats.json")
CALIB_PATH = os.path.join(ARTIFACTS_DIR, "calibration.json")
    
FEATURES = [
    'ALLSKY_SFC_LW_DWN','ALLSKY_SFC_PAR_TOT','ALLSKY_SFC_SW_DIFF','ALLSKY_SFC_SW_DNI',
    'ALLSKY_SFC_SW_DWN','ALLSKY_SFC_UVA','ALLSKY_SFC_UVB','ALLSKY_SFC_UV_INDEX',
    'ALLSKY_SRF_ALB','CLOUD_AMT','CLRSKY_SFC_PAR_TOT','CLRSKY_SFC_SW_DWN',
    'GWETPROF','GWETROOT','GWETTOP','PRECTOTCORR','PRECTOTCORR_SUM','PS','QV2M',
    'RH2M','T2M','T2MDEW','T2MWET','T2M_MAX','T2M_MIN','T2M_RANGE','TOA_SW_DWN','TS'
]

# --- ML ëª¨ë¸ ì´ˆê¸°í™” (ìŠ¤í¬ë¦½íŠ¸ ë¡œë“œ ì‹œ 1íšŒë§Œ ì‹¤í–‰) ---
predictor = None
if ClimatePredictor and os.path.exists(MODEL_PATH):
    try:
        predictor = ClimatePredictor(
            model_path=MODEL_PATH,
            normalizer_stats_path=NORM_PATH,
            calibration_path=CALIB_PATH,
            feature_names=FEATURES,
            sequence_length=6
        )
        print("ClimatePredictor ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ClimatePredictor ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
else:
    print("[ê²½ê³ ] ClimatePredictor ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ì´ ë”ë¯¸ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")

predictors = {
    "climate": None,
    "yield": {}
}

# ê¸°í›„ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
if ClimatePredictor and os.path.exists(MODEL_PATH):
    try:
        predictors["climate"] = ClimatePredictor(
            model_path=MODEL_PATH,
            normalizer_stats_path=NORM_PATH,
            calibration_path=CALIB_PATH,
            feature_names=FEATURES, # ClimatePredictorê°€ ì‚¬ìš©í•˜ëŠ” í”¼ì²˜
            sequence_length=6
        )
        print("ClimatePredictor model loaded successfully.")
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ClimatePredictor ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

# ê° ì‘ë¬¼ë³„ ìƒì‚°ëŸ‰ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
# (ì‹¤ì œë¡œëŠ” ê° ëª¨ë¸ì˜ artifacts ê²½ë¡œë¥¼ ì •í™•íˆ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤)
CROP_MODELS = {
    "MAIZE": MAIZE_PredictorLGBM,
    "WHEAT": WHEAT_PredictorLGBM,
    "RICE": RICE_PredictorLGBM,
    "SOYBEAN": SOYBEAN_PredictorLGBM,
}

for crop_name, PredictorClass in CROP_MODELS.items():
    if PredictorClass:
        try:
            # ê° ì‘ë¬¼ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ crop_nameì„ ì´ìš©í•´ êµ¬ì„± (ì˜ˆ: artifacts/MAIZE/)
            crop_artifact_dir = os.path.join(ARTIFACTS_DIR, crop_name)
            if os.path.exists(crop_artifact_dir):
                predictors["yield"][crop_name] = PredictorClass(artifacts_dir=crop_artifact_dir)
                print(f"{crop_name} Yield Predictor model loaded successfully.")
        except Exception as e:
            print(f"[ì˜¤ë¥˜] {crop_name} Yield Predictor ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")



def load_config(path: str):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Configuration file not found at {path}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON format in {path}")
        return None

def _call_gemini_model(path, contents, model_name="gemini-flash-latest"):
    """Gemini API í˜¸ì¶œì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜."""
    config_data = load_config(path)
    if not config_data:
        return None

    model = genai.GenerativeModel(
        model_name=model_name,
        system_instruction=config_data["system_instruction"]
    )

    generation_config = genai.GenerationConfig(
        response_mime_type=config_data.get("output_mime_type", "text/plain")
    )
    
    # ì˜ˆì œ ì¶”ê°€
    full_contents = []
    for ex in config_data.get("examples", []):
        full_contents.append({"role": "user", "parts": [{"text": ex["input"]}]})
        full_contents.append({"role": "model", "parts": [{"text": json.dumps(ex["output"])}]})
    
    # ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    full_contents.extend(contents)

    try:
        response = model.generate_content(
            contents=full_contents,
            generation_config=generation_config
        )
        # JSON ì‘ë‹µì„ ê°€ì •í•˜ê³  ë¡œë“œ (text/plainì´ë©´ ê·¸ëƒ¥ í…ìŠ¤íŠ¸ê°€ ë  ìˆ˜ ìˆìŒ)
        if config_data.get("output_mime_type") == "application/json":
            return json.loads(response.text)
        return response.text
    except Exception as e:
        print(f"Gemini API call failed for {path}: {e}")
        return None

# --- í•µì‹¬ í•¨ìˆ˜ êµ¬í˜„ ---

def normalizeInput(message):
    """
    1. ì˜¤íƒˆì, ë³„ëª… ë“±ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.
    2. ë¬¸ì¥ ë‚´ì˜ ì¡°ì¹˜ ê°œìˆ˜(action_count)ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # í…œí”Œë¦¿ ì½˜í…ì¸ : ì‚¬ìš©ì ì…ë ¥ë§Œ í¬í•¨
    contents = [{"role": "user", "parts": [{"text": message}]}]
    
    # 'normalizeUserinputPath' ëª¨ë¸ í˜¸ì¶œ
    normalized_data = _call_gemini_model(normalizeUserinputPath, contents)
    
    # ë°˜í™˜ êµ¬ì¡° ì˜ˆì‹œ: {"normalized_text": "ì˜¥ìˆ˜ìˆ˜ ì‹¬ì–´.", "action_count": 1}
    return normalized_data if isinstance(normalized_data, dict) else {"normalized_text": message, "action_count": 0}

def determineQuestionType(normalizedData):
    """
    ì •ê·œí™”ëœ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ Q(ì§ˆë¬¸), I(ëª…ë ¹), O(ê¸°íƒ€) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    message_text = normalizedData.get("normalized_text", "")
    contents = [{"role": "user", "parts": [{"text": message_text}]}]
    
    # 'questionTypeDeterminerPath' ëª¨ë¸ í˜¸ì¶œ
    classification = _call_gemini_model(questionTypeDeterminerPath, contents)
    
    # ë°˜í™˜ êµ¬ì¡° ì˜ˆì‹œ: {"type": "I"}
    return classification if isinstance(classification, dict) else {"type": "O"}

def procedureAnalyzer(sentence):
    """
    ë‹¨ì¼ ëª…ë ¹ì„ ë°›ì•„ ê²Œì„ ì—”ì§„ìš© JSON ê°ì²´(ì˜ë„/ë§¤ê°œë³€ìˆ˜)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ê³¼ë„í•œ ìœ„ì„ ìš”ì²­(ì˜ˆ: 'ì•Œì•„ì„œ í•´ì¤˜')ì€ 'ìœ„ì„_ë¶ˆê°€' ì˜ë„ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
    """
    contents = [{"role": "user", "parts": [{"text": sentence}]}]
    
    # 'procedureAnalyzerPath' ëª¨ë¸ í˜¸ì¶œ
    structured_command = _call_gemini_model(procedureAnalyzerPath, contents)
    
    # ë°˜í™˜ êµ¬ì¡° ì˜ˆì‹œ: {"intent": "ì‹¬ê¸°", "crop": "ì˜¥ìˆ˜ìˆ˜", "target_area": "A-5"}
    return structured_command if isinstance(structured_command, dict) else {"intent": "ë¬´íš¨", "error": "ë¶„ì„ ì‹¤íŒ¨"}

def questionHandler(question_text):
    """
    ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³  ìì—°ì–´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ì´ í•¨ìˆ˜ëŠ” ì§ì ‘ì ìœ¼ë¡œ ê²Œì„ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë¡œì§ì„ ì¡°íšŒí•´ì•¼ í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” GPT ëª¨ë¸ í˜¸ì¶œì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    contents = [{"role": "user", "parts": [{"text": f"ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”: {question_text}"}]}]
    
    # 'feedbackGeneratorPath' ëª¨ë¸ í˜¸ì¶œ (ì§ˆë¬¸ ì‘ë‹µ ëª¨ë“œë¡œ ê°€ì •)
    response_data = _call_gemini_model(feedbackGeneratorPath, contents)

    if isinstance(response_data, str):
        return {"final_response": response_data, "status": "ANSWERED"}
        
    return {"final_response": "ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”.", "status": "ERROR"}


def isCompleted(structured_command):
    """
    ìœ íš¨í•œ êµ¬ì¡°í™” ëª…ë ¹ì„ ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ì— ì „ë‹¬í•˜ê³  ìµœì¢… ì™„ë£Œ í”¼ë“œë°±ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # 1. ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ ì‹¤í–‰ (ì™¸ë¶€ ì‹œìŠ¤í…œ í•¨ìˆ˜ í˜¸ì¶œì„ ê°€ì •)
    try:
        # ì´ í•¨ìˆ˜ëŠ” ì‹¤ì œ ê²Œì„ ë¡œì§(í¬ë ˆë”§ ì°¨ê°, í† ì–‘ ìƒíƒœ ë³€ê²½ ë“±)ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ë°ì´í„°ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        simulation_result = _run_game_simulation(structured_command)
    except Exception as e:
        return {"final_response": f"ëª…ë ¹ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", "status": "ERROR"}

    # 2. í”¼ë“œë°± ìƒì„± (Reasoning/Feedback Generator ì—­í• )
    # ì‹¤í–‰ ê²°ê³¼ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê³  êµìœ¡ì ì¸ í”¼ë“œë°±ì„ ìƒì„±í•©ë‹ˆë‹¤.
    contents = [{"role": "user", "parts": [{"text": f"ì‹¤í–‰ëœ ëª…ë ¹: {structured_command}, ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼: {simulation_result}"}]}]
    
    # 'feedbackGeneratorPath' ëª¨ë¸ í˜¸ì¶œ (ì‹¤í–‰ í”¼ë“œë°± ëª¨ë“œë¡œ ê°€ì •)
    final_feedback = _call_gemini_model(feedbackGeneratorPath, contents)

    return {"final_response": final_feedback, "status": "COMPLETED"}

# llm.pyì˜ _run_game_simulation í•¨ìˆ˜

def _run_game_simulation(command: dict, current_state: dict) -> tuple[dict, dict]:
    """ê¸°í›„ ì˜ˆì¸¡ê³¼ ìƒì‚°ëŸ‰ ì˜ˆì¸¡ ML ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í˜¸ì¶œí•˜ì—¬ ë‹¤ìŒ ìƒíƒœë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤."""
    
    climate_predictor = predictors.get("climate")
    if not climate_predictor:
        raise RuntimeError("ê¸°í›„ ì˜ˆì¸¡ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    new_state = current_state.copy()
    
    # --- 1ë‹¨ê³„: ë‹¤ìŒ ë‹¬ ê¸°í›„ ì˜ˆì¸¡ ---
    climate_history = new_state["climate_history"]
    x_window_climate = climate_history[-6:] # ìµœê·¼ 6ê°œì›” ë°ì´í„°

    # (ì‚¬ìš©ì ëª…ë ¹ì´ ìˆë‹¤ë©´ x_window_climateì— ë¯¸ë¦¬ ë°˜ì˜ - ì´ì „ ë¡œì§ê³¼ ë™ì¼)
    if command.get("intent") == "modify_climate":
        
        # 1. LLMì´ ë¶„ì„í•œ ìƒì„¸ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        params = command.get("parameters", {})
        variable = params.get("variable")  # ì˜ˆ: "T2M" (2m ì˜¨ë„)
        value_str = str(params.get("value", "0")) # ì˜ˆ: "+2" ë˜ëŠ” "-50"

        # 2. ë¬¸ìì—´ë¡œ ëœ ê°’ì„ ìˆ«ì(float)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        try:
            value = float(value_str)
        except (ValueError, TypeError):
            value = 0.0

        # 3. ê°€ì¥ ìµœê·¼ ê¸°í›„ ë°ì´í„°ì— ë³€ê²½ ì‚¬í•­ì„ ì ìš©í•©ë‹ˆë‹¤.
        if variable and value != 0.0:
            # x_window_climateëŠ” ë”•ì…”ë„ˆë¦¬ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
            # ê·¸ ì¤‘ ê°€ì¥ ë§ˆì§€ë§‰ ë‹¬ì˜ ê¸°í›„ ë°ì´í„°(ë”•ì…”ë„ˆë¦¬)ë¥¼ ì•ˆì „í•˜ê²Œ ë³µì‚¬í•©ë‹ˆë‹¤.
            latest_climate_data = x_window_climate[-1].copy()
            
            # í•´ë‹¹ ë³€ìˆ˜(ì˜ˆ: 'T2M')ê°€ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ê°’ì„ ë”í•©ë‹ˆë‹¤.
            if variable in latest_climate_data:
                latest_climate_data[variable] += value
                # ì›ë³¸ ë¦¬ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ ìš”ì†Œë¥¼ ë³€ê²½ëœ ë°ì´í„°ë¡œ êµì²´í•©ë‹ˆë‹¤.
                x_window_climate[-1] = latest_climate_data

    x_window_np = np.array([[month_data[feat] for feat in FEATURES] for month_data in x_window_climate])
    
    mu_o, _, _, _, _, month_next = climate_predictor.predict_next(
        x_window=x_window_np, month_now=new_state["month"],
        lat=new_state["lat"], lon=new_state["lon"], return_original_scale=True
    )
    predicted_climate = {name: float(val) for name, val in zip(FEATURES, mu_o)}

    # --- 2ë‹¨ê³„: ì˜ˆìƒ ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ ---
    yield_prediction = {}
    current_crop = new_state.get("current_crop") # ì˜ˆ: "MAIZE"
    
    if current_crop and current_crop in predictors["yield"]:
        yield_predictor = predictors["yield"][current_crop]
        
        # ìƒì‚°ëŸ‰ ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (ê³¼ê±° 2ê°œì›” + ì˜ˆì¸¡ëœ 1ê°œì›” = ì´ 3ê°œì›”)
        # ì°¸ê³ : ìƒì‚°ëŸ‰ ëª¨ë¸ì´ 3ê°œì›” ë°ì´í„°ë¥¼ ë°›ëŠ”ë‹¤ê³  ê°€ì •. ëª¨ë¸ì— ë”°ë¼ ê¸¸ì´ëŠ” ì¡°ì ˆ í•„ìš”
        x_window_yield = climate_history[-2:] + [predicted_climate]
        
        # ìƒì‚°ëŸ‰ ëª¨ë¸ì€ DataFrameì„ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŒ
        yield_input_df = pd.DataFrame(x_window_yield)
        
        # predict ë©”ì†Œë“œëŠ” DataFrameì„ ë°˜í™˜
        prediction_df = yield_predictor.predict(yield_input_df)
        
        # ê²°ê³¼ DataFrameì—ì„œ ì˜ˆì¸¡ ê°’ ì¶”ì¶œ
        predicted_yield_value = prediction_df["PpA_pred"].iloc[0]
        yield_prediction = {current_crop: predicted_yield_value}

    # --- 3ë‹¨ê³„: ê²Œì„ ìƒíƒœ ì—…ë°ì´íŠ¸ ---
    cost = 50 
    new_state["money"] -= cost
    new_state["turn"] += 1
    new_state["month"] = month_next
    new_state["climate_history"].append(predicted_climate)
    
    # --- 4ë‹¨ê³„: í”¼ë“œë°±ìš© ê²°ê³¼ ìƒì„± ---
    result_for_feedback = {
        "command_status": "SUCCESS",
        "cost": cost,
        "balance": new_state["money"],
        "action_taken": command,
        "prediction": {
            "next_month": month_next,
            "climate": predicted_climate,
            "yield": yield_prediction  # ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€!
        }
    }
    
    return result_for_feedback, new_state


def eventHandler(user_input):
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ ë‹¨ì¼ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ê³  ìµœì¢… ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” ë©”ì¸ íë¦„ ì œì–´ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    
    # 1. ì…ë ¥ ì •ê·œí™” ë° ì¡°ì¹˜ ê°œìˆ˜ í™•ì¸
    normalizedData = normalizeInput(user_input)
    action_count = normalizedData.get("action_count", 0)
    
    # 2. ì§ˆë¬¸ íƒ€ì… í™•ì¸
    sentenceType = determineQuestionType( normalizedData)
    user_type = sentenceType.get("type")
    
    normalized_text = normalizedData.get("normalized_text", user_input)
    
    # --- I. ëª…ë ¹ ì²˜ë¦¬ ('I') ---
    if user_type == 'I':
        # 3. ë‹¨ì¼ ì¡°ì¹˜ ì œì•½ ê²€í† 
        if action_count != 1:
            # ë³µìˆ˜ ì¡°ì¹˜ ëª…ë ¹ -> ê±°ë¶€ ì‘ë‹µ
            return {
                "final_response": "ì ê¹ë§Œìš”. í•œ í„´ì— ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¡°ì¹˜ëŠ” í•˜ë‚˜ë¿ì´ì—ìš”. í•˜ë‚˜ë§Œ ì„ íƒí•´ì„œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?",
                "status": "VIOLATION"
            }
        
        # 4. ë‹¨ì¼ ì¡°ì¹˜ ëª…ë ¹ ë¶„ì„ (procedureAnalyzer)
        structured_command = procedureAnalyzer(normalized_text)
        
        # 5. ê³¼ë„í•œ ìœ„ì„/ë¬´íš¨ ìš”ì²­ ê²€í† 
        if structured_command and structured_command.get("intent") in ["ìœ„ì„_ë¶ˆê°€", "ë¬´íš¨"]:
            # ìœ„ì„ ë¶ˆê°€ ëª…ë ¹ì— ëŒ€í•œ ê±°ë¶€ ì‘ë‹µ
            return {
                "final_response": "ì£„ì†¡í•˜ì§€ë§Œ, ì œê°€ ì„ì˜ë¡œ ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ì—†ì–´ìš”. êµ¬ì²´ì ì¸ ì‘ë¬¼, ì§€ì—­, ê´€ê°œ ë°©ì‹ ë“±ì„ ì§€ì •í•´ ì£¼ì…”ì•¼ í•´ìš”.",
                "status": "VIOLATION"
            }
        
        # 6. ìœ íš¨í•œ ë‹¨ì¼ ëª…ë ¹ ì‹¤í–‰ (isCompleted)
        return isCompleted(structured_command)
        
    # --- II. ì§ˆë¬¸ ì²˜ë¦¬ ('Q') ---
    elif user_type == 'Q':
        # 7. ì§ˆë¬¸ ì²˜ë¦¬ (questionHandler)
        return questionHandler(normalized_text)
        
    # --- III. ê¸°íƒ€ ë°œì–¸ ì²˜ë¦¬ ('O') ---
    else: # user_type == 'O'
        # 8. ì‚¬êµì /ë¬´ì‹œ ì‘ë‹µ
        return {
            "final_response": "ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë†ì—… ì‹œìŠ¤í…œ ê´€ë ¨í•´ì„œ ë„ì›€ì´ í•„ìš”í•  ë•Œ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!",
            "status": "IGNORED"
        }

# --- í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ---

def test_event_handler():
    """í…ŒìŠ¤íŠ¸ ë¬¸ìì—´ì„ ì‚¬ìš©í•˜ì—¬ eventHandlerì˜ íë¦„ì„ í™•ì¸í•˜ëŠ” ë”ë¯¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜."""
    print("--- Event Handler Test Simulation ---")
    
    # GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    api_key = "AIzaSyCSCpQlWCX3ThJTNLzjDYPTBy8gWzqCBTQ"#os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("Error: GEMINI_API_KEY environment variable not set.")
        return

    try:
        genai.configure(api_key=api_key)
        print("Gemini API configured successfully.")
    except Exception as e:
        print(f"Failed to configure Gemini API: {e}")
        return

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° (action_count ë° typeì„ ê°€ì •í•œ ì‘ë‹µ)
    test_inputs = [
        "ë¹„ê°€ ì¢€ ë” ì˜¤ê²Œ í•´",                       # 1. 'I' + ì¡°ì¹˜ 1ê°œ (ë¹„í˜„ì‹¤ì  ëª…ë ¹) -> procedureAnalyzerì—ì„œ í•„í„°ë§ ê°€ëŠ¥
        "ì§€ê¸ˆ ì—¬ê¸°ì„œ ë­í•´ì•¼ë¨",                       # 2. 'Q'
        "ê³¼ê±° ì‹œì  Aë¡œ ëŒì•„ê°€ì„œ ë¬¼ ì¢€ ë” ëŒ€ê³  ìŒ€ë„ ì‹¬ì–´ì¤˜", # 3. 'I' + ì¡°ì¹˜ 2ê°œ -> VIOLATION
        "ì´ì „ ì‹œì ìœ¼ë¡œ ì¢€ ëŒì•„ê°€ì",                 # 4. 'O'
        "ê°•ëƒ‰ì´ ì‹¬ì–´",                            # 5. 'I' + ì¡°ì¹˜ 1ê°œ -> COMPLETED
        "ìŒ€ì´ë‘ ì˜¥ìˆ˜ìˆ˜ ì‹¬ì–´"                         # 6. 'I' + ì¡°ì¹˜ 2ê°œ -> VIOLATION
    ]
    
    print("\n[ì£¼ì˜: ì´ í…ŒìŠ¤íŠ¸ëŠ” JSON ì„¤ì • íŒŒì¼ì˜ ìœ íš¨ì„±ê³¼ ì‹¤ì œ API ì‘ë‹µì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.]")

    for i, item in enumerate(test_inputs):
        print(f"\n--- Test {i+1}: Input: '{item}' ---")
        # ì‹¤ì œ API í˜¸ì¶œì´ í¬í•¨ë˜ë¯€ë¡œ, ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        result = eventHandler(item)
        print(f"Final Result: {json.dumps(result, indent=4, ensure_ascii=False)}")

def start_interactive_mode():
    """ì‚¬ìš©ìì™€ ì§ì ‘ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜."""
    """
    print("--- ë†ì—… ì‹œë®¬ë ˆì´ì…˜ ê²Œì„ AI ë¹„ì„œ ---")
    print("ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    print("(ê²Œì„ì„ ì¢…ë£Œí•˜ë ¤ë©´ 'ì¢…ë£Œ' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”)")
    """

    api_key = "AIzaSyCPbFbFjbAeAbFWkCIeLTnOPy8DQ4YxAvc"
    if not api_key:
        print("\n[ì˜¤ë¥˜] GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ê²Œì„ì„ ì‹œì‘í•˜ê¸° ì „ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    try:
        genai.configure(api_key=api_key)
        print("\nGemini APIê°€ ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜] Gemini API ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return

    while True:
        try:
            # 1. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_input = input("\n> ë‚˜: ")
            
            # 2. ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if user_input.lower() in ["ì¢…ë£Œ", "exit", "quit"]:
                print("ê²Œì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.")
                break
            
            # 3. eventHandlerë¡œ ì…ë ¥ ì²˜ë¦¬
            result = eventHandler(user_input)
            
            # 4. AI ì‘ë‹µ ì¶œë ¥
            ai_response = result.get("final_response", "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            try:
                # ë¨¼ì € ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„ í˜•ì‹ì´ë¼ê³  ê°€ì •í•˜ê³  ë””ì½”ë”©ì„ ì‹œë„í•©ë‹ˆë‹¤.
                decoded_response = ai_response.encode('latin1').decode('unicode_escape')
            except UnicodeEncodeError:
                # ë§Œì•½ ìœ„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ (ì´ë¯¸ ì •ìƒì ì¸ í•œê¸€ì´ë¼ëŠ” ëœ»ì´ë¯€ë¡œ),
                # ì•„ë¬´ ì²˜ë¦¬ ì—†ì´ ì›ë³¸ ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                decoded_response = ai_response

            print(f"ğŸ’¬ AI ë¹„ì„œ: {decoded_response}")

        except KeyboardInterrupt: # Ctrl+C ì…ë ¥ ì‹œ ì¢…ë£Œ
            print("\nê²Œì„ì„ ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"[ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ]: {e}")

if __name__ == "__main__":
    # ë¡œì»¬ì—ì„œë§Œ ì¸í„°ë™í‹°ë¸Œ í…ŒìŠ¤íŠ¸í•  ë•Œ ìˆ˜ë™ìœ¼ë¡œ ì¼œì„¸ìš”
    start_interactive_mode()
    # ë˜ëŠ” test_event_handler()
